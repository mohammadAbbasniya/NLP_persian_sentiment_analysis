{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1a342d",
   "metadata": {},
   "source": [
    "## [phase 1] Search for a good model\n",
    "In this phase we only use train.csv file to find a good model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ecee85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b30cc8",
   "metadata": {},
   "source": [
    "### 1-1 Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be87d48",
   "metadata": {},
   "source": [
    "#### read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41611af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ú©Ø³ Ù…ÛŒØ¯ÙˆÙ†Ù‡ Ú†Ù‡ Ø¬ÙˆØ±ÛŒ Ø§Ø² Ø§ÛŒÙ† Ù‡Ø§ Ù…ÛŒØ´Ù‡ Ø´Ú©Ø§ÛŒØª Ú©Ø±Ø¯ Ù„Ø·Ù...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ø§Ù Ø¨Ø± Ø´Ù‡Ø±Ø¯Ø§Ø±ÛŒ Ú©Ù‡ Ø¯Ø±Ø®Øª Ø±Ø§ ÙˆØ³Ø· Ù…ÛŒØ¯Ø§Ù† Ø§Ù†Ø¯Ø§Ø®ØªÙ‡. Ø·Ø±...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ø®ÛŒÙ„ÛŒ Ø¬Ø§ÛŒ Ø¨Ú©Ø±ÛŒ Ù‡Ø³ Ø­ØªÙ…Ø§ ÛŒÙ‡ Ø³Ø± Ø¨Ø±ÛŒØ¯ğŸ‘Œ</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ø¢Ø¨ Ø¨Ø³ÛŒØ§Ø± Ú©Ø«ÛŒÙ Ø§Ø³ØªØŒ Ù…ØªØ±Ø§Ú˜ Ù‡Ù… Ú©Ù…</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ø§ÙØªØ¶Ø§Ø­ Ú†ÙˆÙ† ÛŒÙ‡ Ø´Ù…Ø§Ø±Ù‡ ØªÙ…Ø§Ø³ Ù†Ø¯Ø§Ø±Ù‡ Ø®ÛŒØ±Ø³Ø±Ø´</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>Ø§ØµÙ„Ø§ Ú©ÛŒÙÛŒØª Ù†Ø¯Ø§Ø±Ù‡ Ø§Ø² Ø³Ø± Ù…Ø¬Ø¨ÙˆØ±ÛŒ Ø§ÙˆÙ…Ø¯ÛŒÙ… Ø§ØªØ§Ù‚ Ú©Ø«ÛŒÙ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539</th>\n",
       "      <td>Ø¨Ø³ÛŒØ§Ø± Ø¹Ø§Ù„ÛŒ Ø¨Ø¯ÙˆÙ† Ø³Ø±Ø¯Ø±Ø¯</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>Ø¨Ø±Ø§ÛŒ Ø²ÛŒØ§Ø±Øª Ùˆ Ø§Ø³ØªØ±Ø§Ø­ØªÛŒ Ú©ÙˆØªØ§Ù‡ Ø®ÙˆØ¨Ù‡ Ø¯Ø± Ø¶Ù…Ù† Ú©ØªØ§Ø¨ Ù...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>Ø¬Ø§ÛŒ Ø®ÙˆØ¨ÛŒ Ù†ÛŒØ³Øª .ÛŒÙ‡ Ø¯ÙˆÙ†Ù‡ Ú©Ø§ÙÛŒ Ø´Ø§Ù¾ Ø§Ù…ÛŒØ± Ø´Ú©Ù„Ø§Øª Ø¨ÙˆØ¯...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2542</th>\n",
       "      <td>Ø®ÛŒÙ„ÛŒ Ø¹Ø§Ù„ÛŒ Ù…Ø®ØµÙˆØµØ§ Ø¯ÙˆØºØ§ÛŒ Ø³Ù‡ Ù„ÛŒØªØ±ÛŒ ØºÙ„ÛŒØ¸Ø´ØŒ Ø¶Ù…Ù†Ø§ Ø¨Ø³...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2543 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment sentiment\n",
       "0     Ú©Ø³ Ù…ÛŒØ¯ÙˆÙ†Ù‡ Ú†Ù‡ Ø¬ÙˆØ±ÛŒ Ø§Ø² Ø§ÛŒÙ† Ù‡Ø§ Ù…ÛŒØ´Ù‡ Ø´Ú©Ø§ÛŒØª Ú©Ø±Ø¯ Ù„Ø·Ù...  Negative\n",
       "1     Ø§Ù Ø¨Ø± Ø´Ù‡Ø±Ø¯Ø§Ø±ÛŒ Ú©Ù‡ Ø¯Ø±Ø®Øª Ø±Ø§ ÙˆØ³Ø· Ù…ÛŒØ¯Ø§Ù† Ø§Ù†Ø¯Ø§Ø®ØªÙ‡. Ø·Ø±...  Negative\n",
       "2                     Ø®ÛŒÙ„ÛŒ Ø¬Ø§ÛŒ Ø¨Ú©Ø±ÛŒ Ù‡Ø³ Ø­ØªÙ…Ø§ ÛŒÙ‡ Ø³Ø± Ø¨Ø±ÛŒØ¯ğŸ‘Œ  Positive\n",
       "3                        Ø¢Ø¨ Ø¨Ø³ÛŒØ§Ø± Ú©Ø«ÛŒÙ Ø§Ø³ØªØŒ Ù…ØªØ±Ø§Ú˜ Ù‡Ù… Ú©Ù…  Negative\n",
       "4                 Ø§ÙØªØ¶Ø§Ø­ Ú†ÙˆÙ† ÛŒÙ‡ Ø´Ù…Ø§Ø±Ù‡ ØªÙ…Ø§Ø³ Ù†Ø¯Ø§Ø±Ù‡ Ø®ÛŒØ±Ø³Ø±Ø´  Negative\n",
       "...                                                 ...       ...\n",
       "2538  Ø§ØµÙ„Ø§ Ú©ÛŒÙÛŒØª Ù†Ø¯Ø§Ø±Ù‡ Ø§Ø² Ø³Ø± Ù…Ø¬Ø¨ÙˆØ±ÛŒ Ø§ÙˆÙ…Ø¯ÛŒÙ… Ø§ØªØ§Ù‚ Ú©Ø«ÛŒÙ...  Negative\n",
       "2539                              Ø¨Ø³ÛŒØ§Ø± Ø¹Ø§Ù„ÛŒ Ø¨Ø¯ÙˆÙ† Ø³Ø±Ø¯Ø±Ø¯  Positive\n",
       "2540  Ø¨Ø±Ø§ÛŒ Ø²ÛŒØ§Ø±Øª Ùˆ Ø§Ø³ØªØ±Ø§Ø­ØªÛŒ Ú©ÙˆØªØ§Ù‡ Ø®ÙˆØ¨Ù‡ Ø¯Ø± Ø¶Ù…Ù† Ú©ØªØ§Ø¨ Ù...  Positive\n",
       "2541  Ø¬Ø§ÛŒ Ø®ÙˆØ¨ÛŒ Ù†ÛŒØ³Øª .ÛŒÙ‡ Ø¯ÙˆÙ†Ù‡ Ú©Ø§ÙÛŒ Ø´Ø§Ù¾ Ø§Ù…ÛŒØ± Ø´Ú©Ù„Ø§Øª Ø¨ÙˆØ¯...  Negative\n",
       "2542  Ø®ÛŒÙ„ÛŒ Ø¹Ø§Ù„ÛŒ Ù…Ø®ØµÙˆØµØ§ Ø¯ÙˆØºØ§ÛŒ Ø³Ù‡ Ù„ÛŒØªØ±ÛŒ ØºÙ„ÛŒØ¸Ø´ØŒ Ø¶Ù…Ù†Ø§ Ø¨Ø³...  Positive\n",
       "\n",
       "[2543 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"sentiment_data/train.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5534e19b",
   "metadata": {},
   "source": [
    "#### ecode sentiment categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0993c2bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negative' 'Negative' 'Positive' ... 'Positive' 'Negative' 'Positive']\n",
      "[0 0 2 ... 2 0 2]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(data.iloc[:,0])\n",
    "y = np.array(data.iloc[:,1])\n",
    "\n",
    "print(y)\n",
    "\n",
    "LE = LabelEncoder()\n",
    "y = LE.fit_transform(y)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cacbe32",
   "metadata": {},
   "source": [
    "#### split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c701ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train : (1703,)\n",
      "X_test  : (840,)\n",
      "y_train : (1703,)\n",
      "y_test  : (840,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "print(f'X_train : {X_train.shape}')\n",
    "print(f'X_test  : {X_test.shape}')\n",
    "print(f'y_train : {y_train.shape}')\n",
    "print(f'y_test  : {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c40344",
   "metadata": {},
   "source": [
    "### 1-2 Text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e78d1e",
   "metadata": {},
   "source": [
    "#### remove emojies\n",
    "Emojies basically show sentiment of writer, should we clean them or no?!\n",
    "Our porpuse is to extract sentiment of a text, but using emojies can be a cheat for out model. I decided to remove all emojies because my model must only rely on the context of a sample not just some random personal emojies! Althoug this decision (clean emolies or not) should be taken by the scenior data scientis of the team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aa9ef9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "vfunc = np.vectorize(lambda s: emoji_pattern.sub(r'', s))\n",
    "X_test = vfunc(X_test)\n",
    "X_train = vfunc(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c3e36e",
   "metadata": {},
   "source": [
    "#### remove unwanted words, punctuations and characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48dd9142",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nlp_files/stop_words-fa.txt', mode='r', encoding='utf-8') as stop_words_file:\n",
    "    stop_words = stop_words_file.read().split('\\n')\n",
    "    \n",
    "with open('nlp_files/stop_puncs-fa.txt', mode='r', encoding='utf-8') as stop_puncs_file:\n",
    "    stop_puncs = stop_puncs_file.read().split('\\n') \n",
    "\n",
    "with open('nlp_files/stop_chars-fa.txt', mode='r', encoding='utf-8') as stop_chars_file:\n",
    "    stop_chars = stop_chars_file.read().split('\\n') \n",
    "\n",
    "def remove_stops(string):\n",
    "    for char in stop_chars:\n",
    "        string = string.replace(char, '') # remove stop-chars\n",
    "        \n",
    "    for punc in stop_puncs:\n",
    "        string = string.replace(punc, ' ') # replace stop-punctuations with space\n",
    "        \n",
    "    words = [word.strip() for word in string.split(' ')] # split string to trimed words \n",
    "    words = list(filter(lambda word: len(word) > 0, words)) # remove empty words\n",
    "    words = list(filter(lambda word: word not in stop_words, words)) # remove stop-words\n",
    "    \n",
    "    string = ' '.join(words)\n",
    "    return string\n",
    "\n",
    "vfunc = np.vectorize(lambda s: remove_stops(s))\n",
    "X_test = vfunc(X_test)\n",
    "X_train = vfunc(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b7c47",
   "metadata": {},
   "source": [
    "### 1-3 Vectorization\n",
    "There are several methods for extracting features of text such as **BAG-OF-WORDS**, **TF-IDF**, **GloVe**, **Word2Vec** and etc.\n",
    "**BAG-OF-WORDS** and **TF-IDF** are based on counting the occurrences of words. In contrast, **GloVe** and **Word2Vec** are basically nural networks which are trained to extract feature. I'm going to apply **TF-IDF** and **Word2Vec** in this phase (finding a good model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b259a7",
   "metadata": {},
   "source": [
    "#### TF-IDF vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "677df412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_TFIDF = TfidfVectorizer()\n",
    "vectorizer_TFIDF.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b998502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tfidf : (1703, 5744)\n",
      "X_test_tfidf  : (840, 5744)\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = vectorizer_TFIDF.transform(X_train).toarray()\n",
    "X_test_tfidf = vectorizer_TFIDF.transform(X_test).toarray()\n",
    "\n",
    "print(f'X_train_tfidf : {X_train_tfidf.shape}')\n",
    "print(f'X_test_tfidf  : {X_test_tfidf.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a68a7df",
   "metadata": {},
   "source": [
    "#### Word2Vec vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56aa419f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x2394db97220>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_size = 1000\n",
    "\n",
    "train_sentences = [sentence.split() for sentence in X_train]\n",
    "vectorizer_W2V = Word2Vec(train_sentences, \n",
    "                          vector_size=vector_size,\n",
    "                          window=5,\n",
    "                          min_count=3, \n",
    "                          workers=4, \n",
    "                          seed=42)\n",
    "vectorizer_W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87e789c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_w2v : (1703, 1000)\n",
      "X_test_w2v  : (840, 1000)\n"
     ]
    }
   ],
   "source": [
    "def vectorize(sentence):\n",
    "    words = sentence.split()\n",
    "    words_vecs = [vectorizer_W2V.wv[word] for word in words if word in vectorizer_W2V.wv]\n",
    "    if len(words_vecs) == 0:\n",
    "        return np.zeros(vector_size)\n",
    "    words_vecs = np.array(words_vecs)\n",
    "    return words_vecs.mean(axis=0)\n",
    "\n",
    "X_train_w2v = np.array([vectorize(sentence) for sentence in X_train])\n",
    "X_test_w2v = np.array([vectorize(sentence) for sentence in X_test])\n",
    "\n",
    "print(f'X_train_w2v : {X_train_w2v.shape}')\n",
    "print(f'X_test_w2v  : {X_test_w2v.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13ae584",
   "metadata": {},
   "source": [
    "### 1-4 Classifiers\n",
    "There are many many different classifiers in machine learning with several techniques. But here, we just employ **KNN**, **SVM** and **XGboost** classifiers which are from three differenet families of classifiers. Each classifier has several parameters, so we need to create each classifier object multiple times with different parameters in order to find the optimal parameter for each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041a04a8",
   "metadata": {},
   "source": [
    "#### KNN classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f14b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_knn_1 = KNeighborsClassifier(n_neighbors=4)\n",
    "clf_knn_2 = KNeighborsClassifier(n_neighbors=8)\n",
    "clf_knn_3 = KNeighborsClassifier(n_neighbors=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb215f",
   "metadata": {},
   "source": [
    "#### SVM classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1be65c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_1 = SVC(kernel='linear')\n",
    "clf_svm_2 = SVC(kernel='poly')\n",
    "clf_svm_3 = SVC(kernel='rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c5058c",
   "metadata": {},
   "source": [
    "#### XGBoost classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32f97c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb_1 = GradientBoostingClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "clf_xgb_2 = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, random_state=42)\n",
    "clf_xgb_3 = GradientBoostingClassifier(n_estimators=150, learning_rate=1.0, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171aaabb",
   "metadata": {},
   "source": [
    "### 1-5 Classification Test\n",
    "Now we have 2 vectorized-samples and 9 classifiers. The last step is to train classifiers and evaluate them with test set to find the best classifier among all. F1-score is used as metric of evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6263da",
   "metadata": {},
   "source": [
    "#### fit and predict on each classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7871867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification using KNeighborsClassifier(n_neighbors=4) ... DONE\n",
      "Classification using KNeighborsClassifier(n_neighbors=8) ... DONE\n",
      "Classification using KNeighborsClassifier(n_neighbors=16) ... DONE\n",
      "Classification using SVC(kernel='linear') ... DONE\n",
      "Classification using SVC(kernel='poly') ... DONE\n",
      "Classification using SVC() ... DONE\n",
      "Classification using GradientBoostingClassifier(learning_rate=1.0, n_estimators=50, random_state=42) ... DONE\n",
      "Classification using GradientBoostingClassifier(learning_rate=1.0, random_state=42) ... DONE\n",
      "Classification using GradientBoostingClassifier(learning_rate=1.0, n_estimators=150, random_state=42) ... DONE\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    clf_knn_1, clf_knn_2, clf_knn_3,\n",
    "    clf_svm_1, clf_svm_2, clf_svm_3,\n",
    "    clf_xgb_1, clf_xgb_2, clf_xgb_3\n",
    "]\n",
    "\n",
    "classifier_names = [\n",
    "    'KNN(n=4)', 'KNN(n=8)', 'KNN(n=16)',\n",
    "    'SVM(linear)', 'SVM(poly)', 'SVM(rbf)',\n",
    "    'XGB(n=50)', 'XGB(n=100)', 'XGB(n=150)', \n",
    "]\n",
    "\n",
    "res = []\n",
    "for i in range(len(classifiers)):\n",
    "    clf = classifiers[i]\n",
    "    print(f'Classification using {clf} ', end='... ')\n",
    "    \n",
    "    clf.fit(X_train_tfidf, y_train) # using TF-IDF vectors\n",
    "    y_pred = clf.predict(X_test_tfidf)  \n",
    "    score_tfidf = f1_score(y_test, y_pred, average='micro')\n",
    "    \n",
    "    clf.fit(X_train_w2v, y_train) # using Word2Vec vectors\n",
    "    y_pred = clf.predict(X_test_w2v)  \n",
    "    score_w2v = f1_score(y_test, y_pred, average='micro')\n",
    "    \n",
    "    print('[DONE]')\n",
    "    res.append([classifier_names[i], score_tfidf, score_w2v])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6cb5bb",
   "metadata": {},
   "source": [
    "#### display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5c17556d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr> <td><h3>Classifiers</h3></td> <td><h3>TF-IDF</h3></td> <td><h3>Word2Vec</h3></td> </tr><tr> <td><h4>KNN(n=4)</h4></td> <td>0.4619</td> <td>0.6131</td> </tr><tr> <td><h4>KNN(n=8)</h4></td> <td>0.6940</td> <td>0.6298</td> </tr><tr> <td><h4>KNN(n=16)</h4></td> <td>0.7524</td> <td>0.6238</td> </tr><tr> <td><h4>SVM(linear)</h4></td> <td>0.7905</td> <td>0.4774</td> </tr><tr> <td><h4>SVM(poly)</h4></td> <td>0.6417</td> <td>0.6452</td> </tr><tr> <td><h4>SVM(rbf)</h4></td> <td>0.7905</td> <td>0.6810</td> </tr><tr> <td><h4>XGB(n=50)</h4></td> <td>0.7214</td> <td>0.6571</td> </tr><tr> <td><h4>XGB(n=100)</h4></td> <td>0.7298</td> <td>0.6786</td> </tr><tr> <td><h4>XGB(n=150)</h4></td> <td>0.7381</td> <td>0.6714</td> </tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "html = \"<table>\"\n",
    "html += f\"<tr> <td><h3>Classifiers</h3></td> <td><h3>TF-IDF</h3></td> <td><h3>Word2Vec</h3></td> </tr>\"\n",
    "for i in range(len(classifiers)):\n",
    "    html += f\"<tr> <td><h4>{res[i][0]}</h4></td> <td>{res[i][1]:.4f}</td> <td>{res[i][2]:.4f}</td> </tr>\"\n",
    "html += \"</table>\"\n",
    "\n",
    "display(HTML(html))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
